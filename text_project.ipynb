{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Text project\n",
    "\n",
    "**Due Thursday, December 13, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to news articles.  The corpus contains ~850K articles from Reuters.  The test set is about 10% of the articles. The data is unextracted in XML files.\n",
    "\n",
    "We're only giving you the code for downloading the data, and how to save the final model. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the project:\n",
    "\n",
    "- One document may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are documents that don't belong to any class, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "- You may use word-embeddings to get better results. For example, you were already using a smaller version of the GloVE  embeddings in exercise 4. Do note that these embeddings take a lot of memory. \n",
    "- In the exercises we used e.g., `torchvision.datasets.MNIST` to handle the loading of the data in suitable batches. Here, you need to handle the dataloading yourself.  The easiest way is probably to create a custom `Dataset`. [See for example here for a tutorial](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torchvision.datasets.utils import download_url\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.helsinki.fi/u/jgpyykko/reuters.zip to train/reuters.zip\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_path = 'train/'\n",
    "\n",
    "dl_file='reuters.zip'\n",
    "dl_url='https://www.cs.helsinki.fi/u/jgpyykko/'\n",
    "zip_path = os.path.join(train_path, dl_file)\n",
    "if not os.path.isfile(zip_path):\n",
    "    download_url(dl_url + dl_file, root=train_path, filename=dl_file, md5=None)\n",
    "with zipfile.ZipFile(zip_path) as zip_f:\n",
    "    zip_f.extractall(train_path)\n",
    "    os.unlink(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloads and extracts the data files into the `train` subdirectory.\n",
    "\n",
    "The files can be found in `train/`, and are named as `19970405.zip`, etc. You will have to manage the content of these zips to get the data. There is a readme which has links to further descriptions on the data.\n",
    "\n",
    "The class labels, or topics, can be found in the readme file called `train/codes.zip`.  The zip contains a file called \"topic_codes.txt\".  This file contains the special codes for the topics (about 130 of them), and the explanation - what each code means.  \n",
    "\n",
    "The XML document files contain the article's headline, the main body text, and the list of topic labels assigned to each article.  You will have to extract the topics of each article from the XML.  For example: \n",
    "&lt;code code=\"C18\"&gt; refers to the topic \"OWNERSHIP CHANGES\" (like a corporate buyout).\n",
    "\n",
    "You should pre-process the XML to extract the words from the article: the &lt;headline&gt; element and the &lt;text&gt;.  You should not need any other parts of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your stuff goes here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "zip_path = \"train/REUTERS_CORPUS_2/\"\n",
    "txt_path = \"train/extracted/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train data\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970728.zip \t( 1 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970404.zip \t( 2 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970415.zip \t( 3 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970412.zip \t( 4 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970529.zip \t( 5 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970805.zip \t( 6 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970730.zip \t( 7 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970604.zip \t( 8 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970725.zip \t( 9 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970710.zip \t( 10 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970414.zip \t( 11 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970705.zip \t( 12 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970630.zip \t( 13 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970626.zip \t( 14 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970806.zip \t( 15 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970622.zip \t( 16 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970517.zip \t( 17 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970817.zip \t( 18 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970708.zip \t( 19 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970522.zip \t( 20 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970715.zip \t( 21 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970720.zip \t( 22 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970807.zip \t( 23 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970610.zip \t( 24 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970422.zip \t( 25 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970810.zip \t( 26 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970403.zip \t( 27 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970518.zip \t( 28 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970503.zip \t( 29 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970512.zip \t( 30 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970406.zip \t( 31 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970511.zip \t( 32 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970608.zip \t( 33 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970802.zip \t( 34 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970707.zip \t( 35 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970409.zip \t( 36 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970628.zip \t( 37 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970429.zip \t( 38 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970811.zip \t( 39 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970424.zip \t( 40 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970523.zip \t( 41 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970601.zip \t( 42 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970507.zip \t( 43 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970411.zip \t( 44 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970516.zip \t( 45 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970408.zip \t( 46 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970723.zip \t( 47 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970815.zip \t( 48 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970726.zip \t( 49 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970703.zip \t( 50 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970712.zip \t( 51 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970616.zip \t( 52 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970819.zip \t( 53 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970419.zip \t( 54 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970407.zip \t( 55 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970506.zip \t( 56 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970615.zip \t( 57 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970612.zip \t( 58 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970812.zip \t( 59 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970706.zip \t( 60 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970721.zip \t( 61 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970526.zip \t( 62 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970621.zip \t( 63 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970509.zip \t( 64 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970814.zip \t( 65 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970813.zip \t( 66 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970417.zip \t( 67 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970513.zip \t( 68 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970505.zip \t( 69 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970519.zip \t( 70 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970624.zip \t( 71 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970731.zip \t( 72 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970702.zip \t( 73 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970525.zip \t( 74 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970423.zip \t( 75 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970418.zip \t( 76 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970614.zip \t( 77 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970502.zip \t( 78 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970613.zip \t( 79 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970623.zip \t( 80 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970816.zip \t( 81 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970607.zip \t( 82 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970727.zip \t( 83 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970515.zip \t( 84 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970711.zip \t( 85 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970504.zip \t( 86 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970704.zip \t( 87 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970717.zip \t( 88 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970524.zip \t( 89 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970416.zip \t( 90 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970405.zip \t( 91 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970713.zip \t( 92 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970605.zip \t( 93 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970508.zip \t( 94 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970804.zip \t( 95 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970625.zip \t( 96 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970611.zip \t( 97 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970701.zip \t( 98 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970618.zip \t( 99 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970528.zip \t( 100 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970803.zip \t( 101 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970425.zip \t( 102 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970627.zip \t( 103 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970722.zip \t( 104 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970602.zip \t( 105 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970714.zip \t( 106 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970809.zip \t( 107 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970718.zip \t( 108 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970401.zip \t( 109 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970603.zip \t( 110 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970620.zip \t( 111 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970428.zip \t( 112 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970402.zip \t( 113 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970426.zip \t( 114 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970501.zip \t( 115 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970606.zip \t( 116 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970716.zip \t( 117 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970427.zip \t( 118 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970527.zip \t( 119 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970521.zip \t( 120 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970617.zip \t( 121 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970531.zip \t( 122 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970413.zip \t( 123 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970514.zip \t( 124 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970801.zip \t( 125 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970421.zip \t( 126 / 127 )\n",
      "..Extracting  train/REUTERS_CORPUS_2/19970724.zip \t( 127 / 127 )\n",
      "Extracting train labels\n"
     ]
    }
   ],
   "source": [
    "# Extracting the zips issued from the dataset file\n",
    "zip_files =  [zip_path+f for f in os.listdir(zip_path) \\\n",
    "              if os.path.isfile(os.path.join(zip_path, f))\\\n",
    "              and f.endswith(\".zip\")]\n",
    "zip_data  = [f for f in zip_files\\\n",
    "             if \"codes\" not in f \n",
    "             and \"dtds\" not in f] # Train data is all files like 132456.zip, excluding code.zip and dtds.zip\n",
    "\n",
    "print(\"Extracting train data\")\n",
    "for i, zfpath in enumerate(zip_data):\n",
    "    print(\"..Extracting \", zfpath, \"\\t(\", i+1, \"/\", len(zip_data),\")\")\n",
    "    with zipfile.ZipFile(zfpath) as zf:\n",
    "        zf.extractall(txt_path)\n",
    "# Exporting codes\n",
    "\n",
    "print(\"Extracting train labels\")\n",
    "with zipfile.ZipFile(zip_path+\"codes.zip\") as zf:\n",
    "    zf.extractall(txt_path+\"labels/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a dictionnary allowing to link code labels to their actual meanings.\n",
    "# For example : the code I50100 stands for \"CONSTRUCTION OF BUILDINGS\"\n",
    "# Codes are the labels we will try to predict.\n",
    "codes_path = txt_path+\"labels/\"\n",
    "code_files = [codes_path+f for f in os.listdir(codes_path) \\\n",
    "              if os.path.isfile(os.path.join(codes_path, f))\n",
    "              and not f.startswith(\"readme\")]\n",
    "\n",
    "code_meaning_dict={} # Creating a dictionnary linking a code label to its meaning.\n",
    "for code_file in code_files:\n",
    "    with open(code_file) as f:\n",
    "        tmp = f.read().split(\"\\n\")\n",
    "    code_file_txt = \"\"\n",
    "    \n",
    "    for line in tmp: \n",
    "        if not line.startswith(\";\") and line!=\"\": # Dropping comments and empty lines\n",
    "            code_tuple = line.split(\"\\t\") # [Code, meaning]\n",
    "            if code_tuple[0] in code_meaning_dict.keys():\n",
    "                print(\"COLLISION!\", file=sys.stderr) # Checking a code doesn't happen twice \n",
    "                                                     # (relatively to the different categories)\n",
    "                # No collision observed ; It is therefore safe to use a single dictionnary for all codes\n",
    "            code_meaning_dict[code_tuple[0]] = code_tuple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "# ElementTree much more performant than xml.dom (100 times)\n",
    "# because based on C. cElementTree deprecated, use ElementTree instead\n",
    "# ref : http://effbot.org/zone/celementtree.htm\n",
    "#       https://docs.python.org/3/library/xml.etree.elementtree.html#module-xml.etree.ElementTree\n",
    "\n",
    "files = [txt_path+f for f in os.listdir(txt_path) \\\n",
    "              if os.path.isfile(os.path.join(txt_path, f)) and f.endswith(\".xml\")]\n",
    "file_to_parse = files[1658] # Define arbritrarily\n",
    "\n",
    "\n",
    "def parse_article(xml_str):\n",
    "    \"\"\" extracts data (headine, text and labels/codes) from a xml formatted article into a dictionnary (json-like) \"\"\"\n",
    "    xml_tree = ET.fromstring(xml_str) # xml tree\n",
    "    res = {}\n",
    "    for node in list(xml_tree):\n",
    "        if node.tag==\"headline\":\n",
    "            res[\"headline\"] = node.text\n",
    "        elif node.tag==\"text\":\n",
    "            res[\"text\"]=\"\"\n",
    "            # The text is included in paragraph subnodes <p>\n",
    "            for paragraph_texts in node.itertext(): #entering <p>\n",
    "                res[\"text\"] += paragraph_texts\n",
    "            res[\"text\"] = res[\"text\"][1:-1] # Text is surrounded by \\n, stripping them\n",
    "\n",
    "        elif node.tag==\"metadata\":\n",
    "            res[\"codes\"] = []\n",
    "            for codesNode in node.findall(\"codes\"): #entering <codes>\n",
    "                for codeNode in codesNode.findall(\"code\"): #entering <code>\n",
    "                    res[\"codes\"].append(codeNode.attrib[\"code\"])\n",
    "    return res\n",
    "\n",
    "file_to_parse = files[1658]\n",
    "\n",
    "with open(file_to_parse) as f:\n",
    "    xml_str = f.read()\n",
    "parsed = parse_article(xml_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: train/extracted/765168newsML.xml\n",
      "HEADLINE: June new-home sales up, inventories lean.\n",
      "TEXT: Brisk new-home sales in June pulled the supply of houses on the market to a four-year low, the Commerce Department said on Wednesday, as an expanding economy braced consumer demand.\n",
      "Sales increased 6.1 percent in June to a seasonally adjusted annual rate of 819,000 units after a steeply revised 1 percent rise to 772,000 in May.\n",
      "Previously, the department said May sales had jumped 7.1 percent to a much higher rate of 825,000. Commerce also revised down sales for March and April from levels that it had estimated earlier.\n",
      "Analysts said widespread revisions, especially the big downward change in May sales, complicated interpetation of the monthly figures. But, they said the positive trend on a year-over-year basis combined with slim inventories increased chances that sales could set a new record this year.\n",
      "\"We are on target to shatter the record for home sales of 757,000 units set last year,\" said Joel Naroff, an economist for First Union Corp. of Charlotte, N.C.\n",
      "Naroff said rising incomes, high levels of consumer confidence and a soaring stock market have bolstered homeowners' buying power.\n",
      "Michael Carliner, an economist with the National Association of Home Builders, said sales for the full year should hit 800,000. The monthly selling pace likely will ease in the second half, he added, but at a lofty enough level to ensure a record for full-year sales.\n",
      "June sales were running 11.9 percent above the level a year earlier, when new homes were selling at a rate of 732,000 a year.\n",
      "The supply of homes on the market was down to 282,000 by the end of June from 283,000 in May -- the lowest inventory level since July 1993, when only 278,000 homes were available.\n",
      "\"Very low inventory levels suggest that housing sales should continue at least at this robust level through the end of the summer and that construction activity will be strong through the fall as they balance inventories,\" said economist Eileen Neely of Fannie Mae, the government-sponsored agency that packages home mortgages for resale to investors.\n",
      "At June's sales pace there was only a 4.2-month supply of new homes on the market, the lowest in 26 years, since a 4.1-month supply in July 1971.\n",
      "Naroff of First Union noted that the housing industry's momentum should reinforce the economy, already in its seventh year of growth since the last recession ended in March 1991.\n",
      "\"As households furnish their newly purchased homes, the impacts on durable goods industries and housing-related industries will be great, with the spending continuing through next spring at least,\" he said.\n",
      "Monthly sales increased in every region during June except the Northeast, where they dropped 21.1 percent to 71,000 after a 3.4 percent increase in May.\n",
      "Midwest sales climbed 7 percent to 153,000 after a 9.2 percent increase in May, and in the South sales were up 9.2 percent following a 1.2 percent gain in May.\n",
      "Sales in the West increased strongly by 12.4 percent to 226,000 in June after a 5.2 percent decline in May.\n",
      "The average home sales price climbed to $176,400 in June from $168,500 in May.\n",
      "LABELS: ['USA', 'I50100', 'I5010022', 'I85000', 'I8500031', 'C31', 'CCAT', 'ECAT']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'code_meaning_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4829f45ee736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mcodes_meaning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"codes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mcodes_meaning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_meaning_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LABELS' MEANING:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodes_meaning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'code_meaning_dict' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"FILE:\", file_to_parse)\n",
    "print(\"HEADLINE:\", parsed[\"headline\"])\n",
    "print(\"TEXT:\", parsed[\"text\"])\n",
    "print(\"LABELS:\", parsed[\"codes\"])\n",
    "codes_meaning = []\n",
    "for code in parsed[\"codes\"]:\n",
    "    codes_meaning.append(code_meaning_dict[code])\n",
    "print(\"LABELS' MEANING:\", codes_meaning)\n",
    "print(xml_str, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1362\n"
     ]
    }
   ],
   "source": [
    "# Number of different labels\n",
    "print(len(code_meaning_dict.keys())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 299772\n",
      "1000 / 299772\n",
      "2000 / 299772\n",
      "3000 / 299772\n",
      "4000 / 299772\n",
      "5000 / 299772\n",
      "6000 / 299772\n",
      "7000 / 299772\n",
      "8000 / 299772\n",
      "9000 / 299772\n",
      "10000 / 299772\n",
      "11000 / 299772\n",
      "12000 / 299772\n",
      "13000 / 299772\n",
      "14000 / 299772\n",
      "15000 / 299772\n",
      "16000 / 299772\n",
      "17000 / 299772\n",
      "18000 / 299772\n",
      "19000 / 299772\n",
      "20000 / 299772\n",
      "21000 / 299772\n",
      "22000 / 299772\n",
      "23000 / 299772\n",
      "24000 / 299772\n",
      "25000 / 299772\n",
      "26000 / 299772\n",
      "27000 / 299772\n",
      "28000 / 299772\n",
      "29000 / 299772\n",
      "30000 / 299772\n",
      "31000 / 299772\n",
      "32000 / 299772\n",
      "33000 / 299772\n",
      "34000 / 299772\n",
      "35000 / 299772\n",
      "36000 / 299772\n",
      "37000 / 299772\n",
      "38000 / 299772\n",
      "39000 / 299772\n",
      "40000 / 299772\n",
      "41000 / 299772\n",
      "42000 / 299772\n",
      "43000 / 299772\n",
      "44000 / 299772\n",
      "45000 / 299772\n",
      "46000 / 299772\n",
      "47000 / 299772\n",
      "48000 / 299772\n",
      "49000 / 299772\n",
      "50000 / 299772\n",
      "51000 / 299772\n",
      "52000 / 299772\n",
      "53000 / 299772\n",
      "54000 / 299772\n",
      "55000 / 299772\n",
      "56000 / 299772\n",
      "57000 / 299772\n",
      "58000 / 299772\n",
      "59000 / 299772\n",
      "60000 / 299772\n",
      "61000 / 299772\n",
      "62000 / 299772\n",
      "63000 / 299772\n",
      "64000 / 299772\n",
      "65000 / 299772\n",
      "66000 / 299772\n",
      "67000 / 299772\n",
      "68000 / 299772\n",
      "69000 / 299772\n",
      "70000 / 299772\n",
      "71000 / 299772\n",
      "72000 / 299772\n",
      "73000 / 299772\n",
      "74000 / 299772\n",
      "75000 / 299772\n",
      "76000 / 299772\n",
      "77000 / 299772\n",
      "78000 / 299772\n",
      "79000 / 299772\n",
      "80000 / 299772\n",
      "81000 / 299772\n",
      "82000 / 299772\n",
      "83000 / 299772\n",
      "84000 / 299772\n",
      "85000 / 299772\n",
      "86000 / 299772\n",
      "87000 / 299772\n",
      "88000 / 299772\n",
      "89000 / 299772\n",
      "90000 / 299772\n",
      "91000 / 299772\n",
      "92000 / 299772\n",
      "93000 / 299772\n",
      "94000 / 299772\n",
      "95000 / 299772\n",
      "96000 / 299772\n",
      "97000 / 299772\n",
      "98000 / 299772\n",
      "99000 / 299772\n",
      "100000 / 299772\n",
      "101000 / 299772\n",
      "102000 / 299772\n",
      "103000 / 299772\n",
      "104000 / 299772\n",
      "105000 / 299772\n",
      "106000 / 299772\n",
      "107000 / 299772\n",
      "108000 / 299772\n",
      "109000 / 299772\n",
      "110000 / 299772\n",
      "111000 / 299772\n",
      "112000 / 299772\n",
      "113000 / 299772\n",
      "114000 / 299772\n",
      "115000 / 299772\n",
      "116000 / 299772\n",
      "117000 / 299772\n",
      "118000 / 299772\n",
      "119000 / 299772\n",
      "120000 / 299772\n",
      "121000 / 299772\n",
      "122000 / 299772\n",
      "123000 / 299772\n",
      "124000 / 299772\n",
      "125000 / 299772\n",
      "126000 / 299772\n",
      "127000 / 299772\n",
      "128000 / 299772\n",
      "129000 / 299772\n",
      "130000 / 299772\n",
      "131000 / 299772\n",
      "132000 / 299772\n",
      "133000 / 299772\n",
      "134000 / 299772\n",
      "135000 / 299772\n",
      "136000 / 299772\n",
      "137000 / 299772\n",
      "138000 / 299772\n",
      "139000 / 299772\n",
      "140000 / 299772\n",
      "141000 / 299772\n",
      "142000 / 299772\n",
      "143000 / 299772\n",
      "144000 / 299772\n",
      "145000 / 299772\n",
      "146000 / 299772\n",
      "147000 / 299772\n",
      "148000 / 299772\n",
      "149000 / 299772\n",
      "150000 / 299772\n",
      "151000 / 299772\n",
      "152000 / 299772\n",
      "153000 / 299772\n",
      "154000 / 299772\n",
      "155000 / 299772\n",
      "156000 / 299772\n",
      "157000 / 299772\n",
      "158000 / 299772\n",
      "159000 / 299772\n",
      "160000 / 299772\n",
      "161000 / 299772\n",
      "162000 / 299772\n",
      "163000 / 299772\n",
      "164000 / 299772\n",
      "165000 / 299772\n",
      "166000 / 299772\n",
      "167000 / 299772\n",
      "168000 / 299772\n",
      "169000 / 299772\n",
      "170000 / 299772\n",
      "171000 / 299772\n",
      "172000 / 299772\n",
      "173000 / 299772\n",
      "174000 / 299772\n",
      "175000 / 299772\n",
      "176000 / 299772\n",
      "177000 / 299772\n",
      "178000 / 299772\n",
      "179000 / 299772\n",
      "180000 / 299772\n",
      "181000 / 299772\n",
      "182000 / 299772\n",
      "183000 / 299772\n",
      "184000 / 299772\n",
      "185000 / 299772\n",
      "186000 / 299772\n",
      "187000 / 299772\n",
      "188000 / 299772\n",
      "189000 / 299772\n",
      "190000 / 299772\n",
      "191000 / 299772\n",
      "192000 / 299772\n",
      "193000 / 299772\n",
      "194000 / 299772\n",
      "195000 / 299772\n",
      "196000 / 299772\n",
      "197000 / 299772\n",
      "198000 / 299772\n",
      "199000 / 299772\n",
      "200000 / 299772\n",
      "201000 / 299772\n",
      "202000 / 299772\n",
      "203000 / 299772\n",
      "204000 / 299772\n",
      "205000 / 299772\n",
      "206000 / 299772\n",
      "207000 / 299772\n",
      "208000 / 299772\n",
      "209000 / 299772\n",
      "210000 / 299772\n",
      "211000 / 299772\n",
      "212000 / 299772\n",
      "213000 / 299772\n",
      "214000 / 299772\n",
      "215000 / 299772\n",
      "216000 / 299772\n",
      "217000 / 299772\n",
      "218000 / 299772\n",
      "219000 / 299772\n",
      "220000 / 299772\n",
      "221000 / 299772\n",
      "222000 / 299772\n",
      "223000 / 299772\n",
      "224000 / 299772\n",
      "225000 / 299772\n",
      "226000 / 299772\n",
      "227000 / 299772\n",
      "228000 / 299772\n",
      "229000 / 299772\n",
      "230000 / 299772\n",
      "231000 / 299772\n",
      "232000 / 299772\n",
      "233000 / 299772\n",
      "234000 / 299772\n",
      "235000 / 299772\n",
      "236000 / 299772\n",
      "237000 / 299772\n",
      "238000 / 299772\n",
      "239000 / 299772\n",
      "240000 / 299772\n",
      "241000 / 299772\n",
      "242000 / 299772\n",
      "243000 / 299772\n",
      "244000 / 299772\n",
      "245000 / 299772\n",
      "246000 / 299772\n",
      "247000 / 299772\n",
      "248000 / 299772\n",
      "249000 / 299772\n",
      "250000 / 299772\n",
      "251000 / 299772\n",
      "252000 / 299772\n",
      "253000 / 299772\n",
      "254000 / 299772\n",
      "255000 / 299772\n",
      "256000 / 299772\n",
      "257000 / 299772\n",
      "258000 / 299772\n",
      "259000 / 299772\n",
      "260000 / 299772\n",
      "261000 / 299772\n",
      "262000 / 299772\n",
      "263000 / 299772\n",
      "264000 / 299772\n",
      "265000 / 299772\n",
      "266000 / 299772\n",
      "267000 / 299772\n",
      "268000 / 299772\n",
      "269000 / 299772\n",
      "270000 / 299772\n",
      "271000 / 299772\n",
      "272000 / 299772\n",
      "273000 / 299772\n",
      "274000 / 299772\n",
      "275000 / 299772\n",
      "276000 / 299772\n",
      "277000 / 299772\n",
      "278000 / 299772\n",
      "279000 / 299772\n",
      "280000 / 299772\n",
      "281000 / 299772\n",
      "282000 / 299772\n",
      "283000 / 299772\n",
      "284000 / 299772\n",
      "285000 / 299772\n",
      "286000 / 299772\n",
      "287000 / 299772\n",
      "288000 / 299772\n",
      "289000 / 299772\n",
      "290000 / 299772\n",
      "291000 / 299772\n",
      "292000 / 299772\n",
      "293000 / 299772\n",
      "294000 / 299772\n",
      "295000 / 299772\n",
      "296000 / 299772\n",
      "297000 / 299772\n",
      "298000 / 299772\n",
      "299000 / 299772\n",
      "CPU times: user 3min 9s, sys: 47.4 s, total: 3min 56s\n",
      "Wall time: 57min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save as a JSON file\n",
    "# Very long (~ 1hour), To be ran only once\n",
    "import json\n",
    "jsontab = []\n",
    "for i, file in enumerate(files):\n",
    "    with open(file, encoding=\"latin-1\") as f: #latin1 in order to process some spanish accents that crash in default utf8\n",
    "        xml_str = f.read()\n",
    "    parsed = parse_article(xml_str)\n",
    "    jsontab.append(parsed)\n",
    "    if i % 1000 == 0:\n",
    "        print(i,\"/\",len(files)-1)\n",
    "\n",
    "import json\n",
    "with open(\"train/database.json\", \"w\") as json_db:\n",
    "    json_db.write(json.dumps(jsontab))\n",
    "del jsontab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read JSON file\n",
    "import json\n",
    "with open(\"train/database.json\") as jsondb_file:\n",
    "    jsondb = json.loads(jsondb_file.read())\n",
    "#print(jsondb[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on a smaller subset to make debugging easier\n",
    "with open(\"train/database_mini.json\", \"w\") as jsondb_file:\n",
    "    for x in jsondb[:100]:\n",
    "            jsondb_file.write(json.dumps(x)+\"\\n\")\n",
    "            # Weird formatting due do pytorch's formatting : succession of JSON lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      ".vector_cache/glove.6B.zip: 0.00B [00:00, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [1:07:13, 214kB/s]                              \n",
      "100%|█████████▉| 399419/400000 [00:30<00:00, 39025.27it/s]"
     ]
    }
   ],
   "source": [
    "# Init glove with 50dim\n",
    "# ref : https://nlp.stanford.edu/projects/glove/\n",
    "from torchtext import datasets, vocab\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU.')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU.')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "glove = vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.data as data\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=lambda x: x.split(), lower=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "\n",
    "# Headline to be added later on\n",
    "datafields = {\"text\" : (\"text\", TEXT),\n",
    "             \"codes\": (\"codes\", LABEL)\n",
    "            }\n",
    "\n",
    "\n",
    "#trn, vld = data.TabularDataset.splits(\n",
    "#               path=\"train/\", # the root directory where the data lies\n",
    "#               train='database_mini.json',\n",
    "#               validation=\"database_mini.json\", # TODO\n",
    "#               format='json',\n",
    "#               skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "#               fields=datafields)\n",
    "\n",
    "\n",
    "trn = data.TabularDataset(path='./train/database_mini.json',\n",
    "                          format=\"json\",\n",
    "                          skip_header=False,\n",
    "                          fields = datafields2)\n",
    "\n",
    "\n",
    "TEXT.build_vocab(trn, vectors=glove)\n",
    "LABEL.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MEX', 'M13', 'M132', 'MCAT']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.examples[50].codes # trn dataset OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here comes the trouble\n",
    "# ref : https://github.com/pytorch/text/blob/master/torchtext/datasets/imdb.py \n",
    "\n",
    "train_iter = data.BucketIterator(\n",
    "     dataset=trn,\n",
    "    batch_size=4,\n",
    "    sort_key=lambda x: len(x.text)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "# same here\n",
    "#train_iter = data.Iterator(\n",
    "#     dataset=trn,\n",
    "#    batch_size=4,\n",
    "#    device =0,\n",
    "#    sort_key=lambda x: len(x.text)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.iterator.BucketIterator at 0x7f310b2be630>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-246-2587f8e67598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# bug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[1;32m    200\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "next(train_iter.__iter__()) # bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-241-2e4c117fefaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[1;32m    200\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "for batch in train_iter:\n",
    "    data = batch.text.transpose(0, 1).to(device) #data\n",
    "    target = (batch.label - 1).to(device) #labels\n",
    "    pass\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network stuff (todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note : Data pre-processing and neural network things could be done into two separate notebooks \n",
    "# files as they will have little common variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "It might be useful to save your model if you want to continue your work later, or use it for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8002db8cca40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\".\n",
    "\n",
    "## Download test set\n",
    "\n",
    "The testset will be made available during the last week before the deadline and can be downloaded in the same way as the training set.\n",
    "\n",
    "## Predict for test set\n",
    "\n",
    "You will be asked to return your predictions a separate test set.  These should be returned as a matrix with one row for each test article.  Each row contains a binary prediction for each label, 1 if it's present in the image, and 0 if not. The order of the labels is the order of the label (topic) codes.\n",
    "\n",
    "An example row could like like this if your system predicts the presense of the second and fourth topic:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0 ...\n",
    "    \n",
    "If you have the matrix prepared in `y` you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('results.txt', y, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
